<p><a href="index.html"><img src="logo.png" alt="Logo" title="" /></a></p>

<h1>Q-Learning</h1>

<p>Dans ce mode, tous les &ecirc;tres-verts sont des 'b&ecirc;tes' g&eacute;r&eacute;es par l'ordinateur, mais celui qui a une bordure noire a un algorithme d'apprentissage qui lui
permet d'&eacute;voluer en fonction de ses exp&eacute;riences.</p>

<p>L'id&eacute;e derri&egrave;re l'apprentissage "Q-Learning" est que l'agent a un <em>&eacute;tat</em> qui, dans notre cas est le compos&eacute;
du niveau de bleu (entre 0 et 7) vu par l'&ecirc;tre-vert, et la derni&egrave;re action qu'il a choisi. L'&ecirc;tre-vert a trois actions possibles pour chaque it&eacute;ration : il peut
avancer, tourner vers la gauche ou tourner vers la droite. Il y a donc 8x3=24 &eacute;tats possibles.</p>

<p>Si nous devions programmer &agrave; la mail un comportement optimal pour
cette situation, nous devrions s&eacute;lectionner la meilleure action pour chaque &eacute;tat. Par exemple, si on ne voir pas de bleu du tout (bleu=0), et que la derni&egrave;re action
&eacute;tait de tourner vers la gauche, alors l'action optimale derait de tourner &agrave; droite.</p>

<p>Pour savoir quelle est l'action optimale dans chaque &eacute;tat, l'&ecirc;tre-vert
lit ce qu'on appelle une Q-Table. Il y stocke la r&eacute;compense attendue pour chaque action et chaque &eacute;tat. par exemple, si on est dans l'&eacute;tat S, l'&eacute;lement lira dans
la Q-Table quelle est la meilleure r&eacute;compense attendue parmi (S;avancer), (S;gauche) et (S;droite). Il s&eacute;lectionnera ensuite l'action ayant la meilleure r&eacute;compense attendue.
<a href="algoChoixAction.html">Infos &agrave; propos de l'algorithme de s&eacute;lection de la meilleure action</a></p>

<p>Bien s&ucirc;r le contenu de la Q-Table est essentiel au processur d'apprentissage. L'id&eacute;e du Q-Learning est de remplir la table pendant que l'&ecirc;tre-vert exp&eacute;rimente des actions : 
au d&eacute;bug la table est vide, donc l'&ecirc;tre-vert fait des actions au hasard, mais quand l'&eacute;l&eacute;ment re&ccedil;oit une r&eacute;compense (quand il touche de l'eau en l'occurence), apr&egrave;s avoir
effectu&eacute; l'action A en l'&eacute;tat S, la case (S;A) de la Q-Table est incr&eacute;ment&eacute;e. Ainsi, la prochaine fois que l'&ecirc;tre vert est dans l'&eacute;tat S, il aura plus de chances d'effectuer
l'action A.</p>

<p>Cependant, comment l'&ecirc;tre-vert est-il capable de prendre des strat&eacute;gies it&eacute;r&eacute;ssantes sur plusieurs it&eacute;rations ? En effet, certaines strat&eacute;gies peuvent &ecirc;tre int&eacute;r&eacute;ssantes, par
exemple avancer quand on voit du bleu) mais ne pas g&eacute;n&eacute;rer de r&eacute;compense imm&eacute;diate. Pour l'instant, seule l'action qui pr&eacute;c&egrave;de imm&eacute;diatement la r&eacute;compense est valoris&eacute;e. C'est
pourquoi m&ecirc;me quand aucune r&eacute;compense imm&eacute;diate n'est re&ccedil;ue apr&egrave;s avoir effectu&eacute;e l'action A en l'&eacute;tat S, la cellule (S;A) de la Q-Table est augment&eacute;e d'une valeur.
Cette valeur d&eacute;pend de la r&eacute;compense attendue maximale que l'&ecirc;tre-vert peut obtenir &agrave; partir du nouvel &eacute;tat S' dans lequel il arrive.</p>

<p>Voici l'algorithme actuel de Q-Learning : </p>

<pre><code>var gamma=0.97;
//a corresponds to the current action

//Compute maximum Q for the next action
var maxQ=0;
for (var i=0; i&lt;3; i++) {
  var cq=q[newState.vision][newState.lastAction][i];
  if (cq&gt;maxQ) maxQ=cq;
}

q[currentState.vision][currentState.lastAction][a]=payoff+gamma*maxQ;
</code></pre>
